{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sd6iFKF2gohh"
      },
      "source": [
        "# **CDS Project: Part 1**\n",
        "\n",
        "*Institute of Software Security (E22)*  \n",
        "*Hamburg University of Technology*  \n",
        "*SoSe 2023*\n",
        "\n",
        "## Learning objectives\n",
        "---\n",
        "\n",
        "- Use a basic Machine Learning (ML) pipeline with pre-trained models.\n",
        "- Build your own data loader.\n",
        "- Load and run a pre-trained ML model.\n",
        "- Evaluate the performance of an ML model.\n",
        "- Calculate and interpret performance metrics.\n",
        "\n",
        "## Materials\n",
        "---\n",
        "\n",
        "- Lecture Slides 1, 2, and 3.\n",
        "- PyTorch Documentation: [Datasets and Data Loaders](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybWt0W4gjbiC"
      },
      "source": [
        "## Project Description\n",
        "---\n",
        "\n",
        "In this project, you are given an ML model that is pre-trained on a vulnerability dataset. The dataset consists of code samples labeled with True or False flags, depending on the presence and absense of a vulnerability. Your goal is to use the pre-trained model to predict if the code samples in the validation set contain vulnerabilities or not and analyse the results. Please proceed to the below tasks. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrciLvqNj96k"
      },
      "source": [
        "###*Task 1*\n",
        "\n",
        "Build a data loader for the validation dataset present in the following path: \"*data_students/student_dataset.hdf5*\". You will be using this dataset to validate the performance of the ML model. The dataset is in HDF5 binary data format. This format is used to store large amount of data. Make sure that you import and familiarise yourself with the right Python libraries to handle HDF5 files. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Eem6AZNyyXsn"
      },
      "outputs": [],
      "source": [
        "# import the necessary libraries to load the data from the specified path.\n",
        "import h5py\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch import nn\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['labels', 'source', 'vectors']\n"
          ]
        }
      ],
      "source": [
        "f = h5py.File(\"./data_students/student_dataset.hdf5\")\n",
        "print(list(f.keys()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dtype object\n"
          ]
        }
      ],
      "source": [
        "dset = f['source']\n",
        "print('dtype', dset.dtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "class VulnDataset(Dataset):\n",
        "  def __init__(self, filepath):\n",
        "    self.path = filepath\n",
        "    self.file_handle = h5py.File(filepath)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.file_handle[\"labels\"])\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return (self.file_handle[\"source\"][idx], self.file_handle[\"labels\"][idx])\n",
        "  \n",
        "  def __getposvuln__(self):\n",
        "    count_true = list(self.file_handle[\"labels\"]).count(True)\n",
        "    return count_true\n",
        "\n",
        "\n",
        "class PreprocessedDataset(Dataset):\n",
        "  def __init__(self, filepath):\n",
        "    self.path = filepath\n",
        "    self.file_handle = h5py.File(filepath)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.file_handle[\"labels\"])\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return (self.file_handle[\"vectors\"][idx], self.file_handle[\"labels\"][idx])\n",
        " \n",
        "dataset_path = Path(\"data_students/student_dataset.hdf5\")\n",
        "val_dataset = VulnDataset(\"data_students/student_dataset.hdf5\")\n",
        "val_loader = DataLoader(val_dataset)\n",
        "\n",
        "process_dataset = PreprocessedDataset(\"data_students/student_dataset.hdf5\")\n",
        "preproc_loader = DataLoader(process_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "283\n",
            "1000\n"
          ]
        }
      ],
      "source": [
        "# print(val_dataset)\n",
        "print(val_dataset.__getposvuln__())\n",
        "print(len(preproc_loader.dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARwcBrbFlMu8"
      },
      "source": [
        "###*Task 2*\n",
        "\n",
        "Generate a table with 10 random samples from the dataset and show their corresponding labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "AuYminA_mTnJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                   0      1\n",
            "0  b'get_charcode(VMG_ uint argc)\\r\\n{\\r\\n    con...  False\n",
            "1  b\"find_open_file_info(char * id) {\\n    unsign...  False\n",
            "2  b'_openipmi_read (ipmi_openipmi_ctx_t ctx,\\n  ...   True\n",
            "3  b'camel_store_get_inbox_folder_sync (CamelStor...   True\n",
            "4  b\"locate_var_of_level_walker(Node *node,\\n\\t\\t...  False\n",
            "5  b'apply(ast_sent* s) {\\n        if (s->get_nod...  False\n",
            "6  b'addr_ston(const struct sockaddr *sa, struct ...   True\n",
            "7  b'printStats(const RunSummary& sol, const Solv...  False\n",
            "8  b'extendtimeline() {\\n  if (timeline.recording...  False\n",
            "9  b'Document(Conf& conf, Encodings& encodings, i...  False\n"
          ]
        }
      ],
      "source": [
        "# TODO: display 10 random samples from the loaded dataset\n",
        "samples = val_dataset[:]\n",
        "df_samples = pd.DataFrame(samples).transpose()\n",
        "print(df_samples.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da5YCWVkmUL2"
      },
      "source": [
        "###*Task 3*\n",
        "\n",
        "Inspect the dataset and answer the following questions:\n",
        "1.  How many samples are in the dataset?\n",
        "2. How many positive examples (vulnerability-labeled instances) are in the dataset?\n",
        "3. What is the vulnerable/non-vulnerable ratio?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "LDpozMCfnnJg"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "length of dataset is 1000\n",
            "Number of positive examples in dataset: 283\n",
            "The ratio vuln/non-vuln in dataset is 0.283\n"
          ]
        }
      ],
      "source": [
        "# TODO: inspect and understand the loaded dataset\n",
        "length_dataset = val_dataset.__len__()\n",
        "print(f'length of dataset is {length_dataset}')\n",
        "count_vuln = val_dataset.__getposvuln__()\n",
        "print(f'Number of positive examples in dataset: {count_vuln}')\n",
        "print(f'The ratio vuln/non-vuln in dataset is {count_vuln/length_dataset}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UivWlO3dnngr"
      },
      "source": [
        "###*Task 4*\n",
        "\n",
        "Load and run the following pre-trained neural network model called VulnPredictionModel. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Jex8XdkFJhb"
      },
      "source": [
        "``` python \n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RrGtLkpEzKu"
      },
      "source": [
        "``` python\n",
        "from torch import nn\n",
        "\n",
        "class VulnPredictModel(nn.Module):\n",
        "    # intialize the model architecture\n",
        "    def __init__(self):\n",
        "      super().__init__()\n",
        "      self.flatten = nn.Flatten()\n",
        "      self.linear_stack = nn.Sequential(\n",
        "         nn.Linear(768, 64),\n",
        "         nn.ReLU(),\n",
        "         nn.Linear(64, 64),\n",
        "         nn.ReLU(),\n",
        "         nn.Linear(64, 1),\n",
        "         nn.Sigmoid()\n",
        "      )\n",
        "\n",
        "      # forward propagation\n",
        "      def forward(self, x):\n",
        "        pred = self.linear_stack(x)\n",
        "        return pred\n",
        "      \n",
        "\n",
        "# TODO: intialize and load the model\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cpu device\n"
          ]
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "class VulnPredictModel(nn.Module):\n",
        "    # intialize the model architecture\n",
        "    def __init__(self):\n",
        "      super().__init__()\n",
        "      self.flatten = nn.Flatten()\n",
        "      self.linear_stack = nn.Sequential(\n",
        "         nn.Linear(768, 64),\n",
        "         nn.ReLU(),\n",
        "         nn.Linear(64, 64),\n",
        "         nn.ReLU(),\n",
        "         nn.Linear(64, 1),\n",
        "         nn.Sigmoid()\n",
        "      )\n",
        "\n",
        "    # forward propagation\n",
        "    def forward(self, x):\n",
        "      pred = self.linear_stack(x)\n",
        "      return pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# TODO: intialize and load the model\n",
        "model = VulnPredictModel()\n",
        "model.load_state_dict(torch.load(\"model_2023-03-28_20-03.pth\", map_location=torch.device('cpu')))\n",
        "# model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-A9M9ID0n2Gx"
      },
      "source": [
        "###*Task 5*\n",
        "\n",
        "Make a prediction on the provided dataset and compute the following values:\n",
        "- True Positives\n",
        "- True Negatives\n",
        "- False Positives\n",
        "- False Negatives"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "R8KdeQ2Rn-2Z"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Error: \n",
            "Accuracy: 71.7%, \n",
            "Average loss: 0.567931 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# TODO: make the prediction for all the samples in the validation set.\n",
        "\n",
        "predictions = []\n",
        "false_pos, false_neg, true_pos, true_neg = 0, 0, 0, 0\n",
        "loss, correct = 0, 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for ip, op in preproc_loader:\n",
        "        ip, op = ip.to(device), op.to(device)\n",
        "        pred = model(ip)\n",
        "        predictions.extend(pred)\n",
        "        op = torch.tensor(float(op))\n",
        "        # print(pred, op)\n",
        "        if pred - op >= 0.5:\n",
        "            false_pos += 1\n",
        "        elif pred - op >= 0:\n",
        "            true_neg += 1\n",
        "        elif pred - op >= -0.5:\n",
        "            true_pos += 1\n",
        "        else:\n",
        "            false_neg += 1\n",
        "        loss_func = nn.BCELoss()\n",
        "        loss += loss_func(pred, np.reshape(op, (1,1,1))).item()\n",
        "        correct += (pred.argmax(1) == op).type(torch.float).sum().item()\n",
        "loss /= len(preproc_loader)\n",
        "correct /= len(preproc_loader.dataset)\n",
        "print(f\"Test Error: \\nAccuracy: {(100*correct):>0.1f}%, \\nAverage loss: {loss:>8f} \\n\")\n",
        "\n",
        "# print(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True Positives: 20\n",
            "True Negatives: 716\n",
            "False Postives: 1\n",
            "False Negatives: 263\n"
          ]
        }
      ],
      "source": [
        "# TODO: compute true positives, true negatives, false postives and false negatives.\n",
        "print(f'True Positives: {true_pos}\\nTrue Negatives: {true_neg}\\nFalse Postives: {false_pos}\\nFalse Negatives: {false_neg}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TaFHwiVwow7s"
      },
      "source": [
        "### *Task 6*\n",
        "\n",
        "Compute the corresponding performance metrics **manually** (do not use PyTorch's predefined metrics):\n",
        "- Accuracy\n",
        "- Precision\n",
        "- Recall\n",
        "- F1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "KE2daH3LpGEc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy 0.736\n",
            "Precision 0.9523809523809523 \n",
            "Recall 0.0706713780918728 \n",
            "F1 Score 0.13157894736842105\n"
          ]
        }
      ],
      "source": [
        "# TODO: calculate accuracy\n",
        "\n",
        "accuracy = (true_neg + true_pos) / (true_neg + true_pos + false_neg + false_pos)\n",
        "\n",
        "# TODO: calculate precision\n",
        "\n",
        "precision = true_pos / (true_pos + false_pos)\n",
        "\n",
        "# TODO: calculate recall\n",
        "\n",
        "recall = true_pos / (true_pos + false_neg)\n",
        "\n",
        "# TODO: calculate F1-score\n",
        "\n",
        "f1score = 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "print(f'Accuracy {accuracy}\\nPrecision {precision} \\nRecall {recall} \\nF1 Score {f1score}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdIkKUPlpGjv"
      },
      "source": [
        "### *Task 7*\n",
        "\n",
        "Based on your performance metrics, answer the following questions:\n",
        "\n",
        "- Explain the impact of accuracy vs. F1 score.\n",
        "- In this particular problem, which metric one should focus more on?\n",
        "- Is there a better metric suitable for the use case of vulnerability prediction? Why?\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
