{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sd6iFKF2gohh"
   },
   "source": [
    "# **CDS Project: Part 1**\n",
    "\n",
    "*Institute of Software Security (E22)*  \n",
    "*Hamburg University of Technology*  \n",
    "*SoSe 2023*\n",
    "\n",
    "## Learning objectives\n",
    "---\n",
    "\n",
    "- Use a basic Machine Learning (ML) pipeline with pre-trained models.\n",
    "- Build your own data loader.\n",
    "- Load and run a pre-trained ML model.\n",
    "- Evaluate the performance of an ML model.\n",
    "- Calculate and interpret performance metrics.\n",
    "\n",
    "## Materials\n",
    "---\n",
    "\n",
    "- Lecture Slides 1, 2, and 3.\n",
    "- PyTorch Documentation: [Datasets and Data Loaders](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ybWt0W4gjbiC"
   },
   "source": [
    "## Project Description\n",
    "---\n",
    "\n",
    "In this project, you are given an ML model that is pre-trained on a vulnerability dataset. The dataset consists of code samples labeled with True or False flags, depending on the presence and absense of a vulnerability. Your goal is to use the pre-trained model to predict if the code samples in the validation set contain vulnerabilities or not and analyse the results. Please proceed to the below tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IrciLvqNj96k"
   },
   "source": [
    "###*Task 1*\n",
    "\n",
    "Build a data loader for the validation dataset present in the following path: \"*data/dataset.hdf5*\". You will be using this dataset to validate the performance of the ML model. The dataset is in HDF5 binary data format. This format is used to store large amount of data. Make sure that you import and familiarise yourself with the right Python libraries to handle HDF5 files. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Eem6AZNyyXsn"
   },
   "outputs": [],
   "source": [
    "import h5py    \n",
    "import numpy as np   \n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KeysViewHDF5 ['labels', 'source', 'vectors']>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = h5py.File(\"data/dataset.hdf5\",'r+')  \n",
    "f.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CyberSecurityDataset(Dataset):\n",
    "    def __init__(self, path):\n",
    "        self.file = h5py.File(path, 'r+')\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.file[\"labels\"])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (self.file[\"source\"][idx], self.file[\"labels\"][idx])\n",
    "    \n",
    "    def __getlabels__(self):\n",
    "        return self.file[\"labels\"][()]\n",
    "    \n",
    "    def __getsources__(self):\n",
    "        return self.file[\"source\"][()]\n",
    "    \n",
    "class PreprocessDataset(Dataset):\n",
    "    def __init__(self, path):\n",
    "        self.file = h5py.File(path, 'r+')\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.file[\"labels\"])\n",
    "    \n",
    "    def __getvectors__(self):\n",
    "        return (self.file[\"vectors\"][()])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (torch.Tensor(self.file[\"vectors\"][idx]), self.file[\"labels\"][idx])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CyberSecurityDataset(\"data/dataset.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ARwcBrbFlMu8"
   },
   "source": [
    "###*Task 2*\n",
    "\n",
    "Generate a table with 10 random samples from the dataset and show their corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "AuYminA_mTnJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              source  label\n",
      "0  b'_gf_log_eh (const char *function, const char...   True\n",
      "1  b'ASU_141( const int& h, const int& k, const i...  False\n",
      "2  b\"container() const\\n{\\n    // This method is ...  False\n",
      "3  b'unconvert(obj)\\nCcWnnObject obj;\\n{\\n    if ...  False\n",
      "4  b'git_odb_object__free(void *object)\\n{\\n\\tif ...  False\n",
      "5  b\"il_separator(c)\\n    char c;\\n{\\n    if ((c ...  False\n",
      "6  b'attributesSigned(DcmItem& item, DcmAttribute...  False\n",
      "7  b'AddSharedLibNoSOName(std::string const& item...  False\n",
      "8  b'xen_vcpu_notify_restore(void *data)\\n{\\n\\t/*...  False\n",
      "9  b'LoadNyquistEffect(wxString fname)\\n{\\n   Eff...  False\n"
     ]
    }
   ],
   "source": [
    "import random as randn\n",
    "df=pd.DataFrame(columns=[\"source\", \"label\"])\n",
    "\n",
    "for i in range(10):\n",
    "    index = randn.randint(0, dataset.__len__())\n",
    "    df.loc[len(df.index)] = [dataset.__getitem__(index)[0], dataset.__getitem__(index)[1]] \n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "da5YCWVkmUL2"
   },
   "source": [
    "###*Task 3*\n",
    "\n",
    "Inspect the dataset and answer the following questions:\n",
    "1.  How many samples are in the dataset?\n",
    "2. How many positive examples (vulnerability-labeled instances) are in the dataset?\n",
    "3. What is the vulnerable/non-vulnerable ratio?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "LDpozMCfnnJg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1000 samples in the dataset\n",
      "There are 283 positive examples in the dataset\n",
      "The ratio between positive and negative is 39.47001394700139%\n"
     ]
    }
   ],
   "source": [
    "# TODO: inspect and understand the loaded dataset\n",
    "print(f\"There are {dataset.__len__()} samples in the dataset\")\n",
    "print(f\"There are {dataset.__getlabels__().sum()} positive examples in the dataset\")\n",
    "print(f\"The ratio between positive and negative is {dataset.__getlabels__().sum()/(dataset.__len__()-dataset.__getlabels__().sum())*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UivWlO3dnngr"
   },
   "source": [
    "###*Task 4*\n",
    "\n",
    "Load and run the following pre-trained neural network model called VulnPredictionModel. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Jex8XdkFJhb"
   },
   "source": [
    "``` python \n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9RrGtLkpEzKu"
   },
   "source": [
    "``` python\n",
    "from torch import nn\n",
    "\n",
    "class VulnPredictModel(nn.Module):\n",
    "    # intialize the model architecture\n",
    "    def __init__(self):\n",
    "      super().__init__()\n",
    "      self.flatten = nn.Flatten()\n",
    "      self.linear_stack = nn.Sequential(\n",
    "         nn.Linear(768, 64),\n",
    "         nn.ReLU(),\n",
    "         nn.Linear(64, 64),\n",
    "         nn.ReLU(),\n",
    "         nn.Linear(64, 1),\n",
    "         nn.Sigmoid()\n",
    "      )\n",
    "\n",
    "      # forward propagation\n",
    "      def forward(self, x):\n",
    "        pred = self.linear_stack(x)\n",
    "        return pred\n",
    "      \n",
    "\n",
    "# TODO: intialize and load the model\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class VulnPredictModel(nn.Module):\n",
    "    # intialize the model architecture\n",
    "    def __init__(self):\n",
    "      super().__init__()\n",
    "      self.flatten = nn.Flatten()\n",
    "      self.linear_stack = nn.Sequential(\n",
    "         nn.Linear(768, 64),\n",
    "         nn.ReLU(),\n",
    "         nn.Linear(64, 64),\n",
    "         nn.ReLU(),\n",
    "         nn.Linear(64, 1),\n",
    "         nn.Sigmoid()\n",
    "      )\n",
    "\n",
    "      # forward propagation\n",
    "    def forward(self, x):\n",
    "        pred = self.linear_stack(x)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VulnPredictModel(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (linear_stack): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=64, out_features=1, bias=True)\n",
       "    (5): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = VulnPredictModel()\n",
    "model.load_state_dict(torch.load(\"model_2023-03-28_20-03.pth\", map_location=torch.device('cpu'))) \n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-A9M9ID0n2Gx"
   },
   "source": [
    "###*Task 5*\n",
    "\n",
    "Make a prediction on the provided dataset and compute the following values:\n",
    "- True Positives\n",
    "- True Negatives\n",
    "- False Positives\n",
    "- False Negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "R8KdeQ2Rn-2Z"
   },
   "outputs": [],
   "source": [
    "# TODO: makethe prediction for all the samples in the validation set.\n",
    "processed_dataset = PreprocessDataset(\"data/dataset.hdf5\")\n",
    "for i in range(processed_dataset.__len__()):\n",
    "    data, label = processed_dataset.__getitem__(i)\n",
    "    prediction = model(data)\n",
    "\n",
    "# todo: compute true positives, true negatives, false postives and false negatives.\n",
    "    \n",
    "FN = 0\n",
    "FP = 0\n",
    "TP = 0\n",
    "TN = 0\n",
    "for i in range(processed_dataset.__len__()):\n",
    "    data, label = processed_dataset.__getitem__(i)\n",
    "    prediction = model(data)\n",
    "    if prediction > 0.5:\n",
    "        if label : \n",
    "            TP+=1\n",
    "        else:\n",
    "            FP+=1\n",
    "    else:\n",
    "        if label:\n",
    "            FN+=1\n",
    "        else:\n",
    "            TN+=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of false positive is 1\n",
      "The number of false negative is 263\n",
      "The number of true positive is 20\n",
      "The number of true negative is 716\n"
     ]
    }
   ],
   "source": [
    "print(f\"The number of false positive is {FP}\")\n",
    "print(f\"The number of false negative is {FN}\")\n",
    "print(f\"The number of true positive is {TP}\")\n",
    "print(f\"The number of true negative is {TN}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TaFHwiVwow7s"
   },
   "source": [
    "### *Task 6*\n",
    "\n",
    "Compute the corresponding performance metrics **manually** (do not use PyTorch's predefined metrics):\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "KE2daH3LpGEc"
   },
   "outputs": [],
   "source": [
    "# TODO: calculate accuracy\n",
    "accuracy = (TP + TN) / (TP + TN + FP + FN) if (TP + TN + FP + FN) > 0 else 0\n",
    "# TODO: calculate precision\n",
    "precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "# TODO: calculate recall\n",
    "recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "# TODO: calculate F1-score\n",
    "f1_score = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.736\n",
      "Precision: 0.9523809523809523\n",
      "Recall: 0.0706713780918728\n",
      "F1 Score: 0.13157894736842105\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy: {accuracy}\" )\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kdIkKUPlpGjv"
   },
   "source": [
    "### *Task 7*\n",
    "\n",
    "Based on your performance metrics, answer the following questions:\n",
    "\n",
    "- Explain the impact of accuracy vs. F1 score.\n",
    "- In this particular problem, which metric one should focus more on?\n",
    "- Is there a better metric suitable for the use case of vulnerability prediction? Why?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy mesures the overall performance of a model whereas F1 score focuses on the impact of false positive and false negative.\n",
    "In this problem, there is a huge impact in case of false negative sample because that leads to a vulnerable system. Thus, F1 is more appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
